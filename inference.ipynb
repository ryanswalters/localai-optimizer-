#!/bin/bash
set -e

echo "========================================="
echo "Complete AI Stack Setup Script"
echo "Qwen2.5-Coder-32B (128K context) + Embeddings + Qdrant"
echo "========================================="
echo ""

# Color codes for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Function to print colored output
print_status() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

print_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

print_section() {
    echo -e "\n${BLUE}=========================================${NC}"
    echo -e "${BLUE}$1${NC}"
    echo -e "${BLUE}=========================================${NC}\n"
}

# Check if running as root
if [[ $EUID -ne 0 ]]; then
   print_error "This script must be run as root (use sudo)"
   exit 1
fi

# Detect OS
if [ -f /etc/os-release ]; then
    . /etc/os-release
    OS=$ID
    VERSION=$VERSION_ID
else
    print_error "Cannot detect OS. /etc/os-release not found."
    exit 1
fi

print_status "Detected OS: $OS $VERSION"

print_section "Step 1: System Update"

# Update system
print_status "Updating system packages..."
if [[ "$OS" == "ubuntu" ]] || [[ "$OS" == "debian" ]]; then
    apt-get update -y
    apt-get upgrade -y
    apt-get install -y curl wget git software-properties-common apt-transport-https \
        ca-certificates gnupg lsb-release python3 python3-pip jq
elif [[ "$OS" == "centos" ]] || [[ "$OS" == "rhel" ]] || [[ "$OS" == "rocky" ]]; then
    yum update -y
    yum install -y curl wget git yum-utils python3 python3-pip jq
else
    print_error "Unsupported OS: $OS"
    exit 1
fi

print_section "Step 2: Docker Installation"

# Install Docker
print_status "Installing Docker..."
if ! command -v docker &> /dev/null; then
    if [[ "$OS" == "ubuntu" ]] || [[ "$OS" == "debian" ]]; then
        # Add Docker's official GPG key
        install -m 0755 -d /etc/apt/keyrings
        curl -fsSL https://download.docker.com/linux/$OS/gpg | gpg --dearmor -o /etc/apt/keyrings/docker.gpg
        chmod a+r /etc/apt/keyrings/docker.gpg

        # Add the repository to Apt sources
        echo \
          "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/$OS \
          $(lsb_release -cs) stable" | tee /etc/apt/sources.list.d/docker.list > /dev/null

        apt-get update -y
        apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
    elif [[ "$OS" == "centos" ]] || [[ "$OS" == "rhel" ]] || [[ "$OS" == "rocky" ]]; then
        yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
        yum install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
    fi

    # Start and enable Docker
    systemctl start docker
    systemctl enable docker
    print_status "Docker installed successfully"
else
    print_status "Docker is already installed"
fi

docker --version

print_section "Step 3: NVIDIA Container Toolkit"

# Install NVIDIA Container Toolkit
print_status "Installing NVIDIA Container Toolkit..."
if ! dpkg -l | grep -q nvidia-container-toolkit 2>/dev/null && ! rpm -qa | grep -q nvidia-container-toolkit 2>/dev/null; then
    if [[ "$OS" == "ubuntu" ]] || [[ "$OS" == "debian" ]]; then
        # Add NVIDIA Container Toolkit repository
        distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
        curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
        curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | \
            sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
            tee /etc/apt/sources.list.d/nvidia-container-toolkit.list

        apt-get update -y
        apt-get install -y nvidia-container-toolkit
    elif [[ "$OS" == "centos" ]] || [[ "$OS" == "rhel" ]] || [[ "$OS" == "rocky" ]]; then
        distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
        curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.repo | \
            tee /etc/yum.repos.d/nvidia-container-toolkit.repo
        yum install -y nvidia-container-toolkit
    fi

    # Configure Docker to use NVIDIA runtime
    nvidia-ctk runtime configure --runtime=docker
    systemctl restart docker
    print_status "NVIDIA Container Toolkit installed successfully"
else
    print_status "NVIDIA Container Toolkit is already installed"
fi

# Test NVIDIA Docker
print_status "Testing NVIDIA Docker runtime..."
if docker run --rm --gpus all nvidia/cuda:12.0.0-base-ubuntu22.04 nvidia-smi; then
    print_status "âœ“ NVIDIA Docker runtime is working correctly"
else
    print_error "NVIDIA Docker runtime test failed. Please check your NVIDIA drivers."
    exit 1
fi

print_section "Step 4: Python Dependencies"

# Install Python packages for management scripts
print_status "Installing Python packages..."
pip3 install --upgrade pip
pip3 install requests openai qdrant-client sentence-transformers

# Install Docker Compose (standalone)
print_status "Ensuring Docker Compose is available..."
if ! command -v docker-compose &> /dev/null; then
    DOCKER_COMPOSE_VERSION=$(curl -s https://api.github.com/repos/docker/compose/releases/latest | grep 'tag_name' | cut -d\" -f4)
    curl -L "https://github.com/docker/compose/releases/download/${DOCKER_COMPOSE_VERSION}/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
    chmod +x /usr/local/bin/docker-compose
fi
docker compose version || docker-compose --version

print_section "Step 5: Project Setup"

# Create project directory
PROJECT_DIR="/opt/ai-stack"
print_status "Creating project directory at $PROJECT_DIR..."
mkdir -p $PROJECT_DIR/{data/qdrant,configs}
cd $PROJECT_DIR

# Create .env file
print_status "Creating configuration file..."
cat > .env <<EOF
# ==========================================
# LLM Configuration (Qwen2.5-Coder-32B)
# ==========================================
LLM_MODEL_NAME=Qwen/Qwen2.5-Coder-32B-Instruct
LLM_GPU_MEMORY_UTILIZATION=0.85
LLM_MAX_MODEL_LEN=131072
LLM_TENSOR_PARALLEL_SIZE=1
LLM_DTYPE=float16
LLM_PORT=8000

# ==========================================
# Embedding Configuration
# ==========================================
EMBEDDING_MODEL_NAME=BAAI/bge-large-en-v1.5
EMBEDDING_PORT=8001
EMBEDDING_MAX_LENGTH=512

# ==========================================
# Qdrant Vector Database
# ==========================================
QDRANT_PORT=6333
QDRANT_GRPC_PORT=6334

# ==========================================
# HuggingFace Configuration
# ==========================================
# Get your token from: https://huggingface.co/settings/tokens
# Recommended for faster downloads and access to gated models
HF_TOKEN=

# ==========================================
# Advanced Settings
# ==========================================
TRUST_REMOTE_CODE=true
EOF

print_status "Created .env file at $PROJECT_DIR/.env"

print_section "Step 6: Docker Compose Configuration"

# Create docker-compose.yml
print_status "Creating Docker Compose configuration..."
cat > docker-compose.yml <<'COMPOSE_EOF'
version: '3.8'

services:
  # ==========================================
  # Qwen2.5-Coder-32B LLM Service (128K context)
  # ==========================================
  qwen-llm:
    image: vllm/vllm-openai:latest
    container_name: qwen-llm
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - HF_TOKEN=${HF_TOKEN}
      - VLLM_LOGGING_LEVEL=INFO
    ports:
      - "${LLM_PORT}:8000"
    volumes:
      - huggingface-cache:/root/.cache/huggingface
    command: >
      --model ${LLM_MODEL_NAME}
      --dtype ${LLM_DTYPE}
      --gpu-memory-utilization ${LLM_GPU_MEMORY_UTILIZATION}
      --max-model-len ${LLM_MAX_MODEL_LEN}
      --tensor-parallel-size ${LLM_TENSOR_PARALLEL_SIZE}
      --trust-remote-code
      --served-model-name qwen-coder
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

  # ==========================================
  # Embedding Service
  # ==========================================
  embeddings:
    image: ghcr.io/huggingface/text-embeddings-inference:1.5
    container_name: embeddings
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - HF_TOKEN=${HF_TOKEN}
    ports:
      - "${EMBEDDING_PORT}:80"
    volumes:
      - embeddings-cache:/data
    command: >
      --model-id ${EMBEDDING_MODEL_NAME}
      --max-client-batch-size 512
      --max-batch-tokens 16384
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # ==========================================
  # Qdrant Vector Database
  # ==========================================
  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    ports:
      - "${QDRANT_PORT}:6333"
      - "${QDRANT_GRPC_PORT}:6334"
    volumes:
      - ./data/qdrant:/qdrant/storage
    environment:
      - QDRANT__SERVICE__GRPC_PORT=6334
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

volumes:
  huggingface-cache:
    driver: local
  embeddings-cache:
    driver: local
COMPOSE_EOF

print_status "Docker Compose configuration created"

print_section "Step 7: Management Scripts"

# Create comprehensive test script
print_status "Creating test script..."
cat > test_services.py <<'TEST_EOF'
#!/usr/bin/env python3
"""
Comprehensive test suite for AI Stack
Tests: LLM, Embeddings, Qdrant
"""
import requests
import json
import sys
import time
from typing import Dict, Any

# Service URLs
LLM_URL = "http://localhost:8000"
EMBEDDING_URL = "http://localhost:8001"
QDRANT_URL = "http://localhost:6333"

class Colors:
    GREEN = '\033[92m'
    RED = '\033[91m'
    YELLOW = '\033[93m'
    BLUE = '\033[94m'
    END = '\033[0m'

def print_header(text: str):
    print(f"\n{Colors.BLUE}{'='*80}{Colors.END}")
    print(f"{Colors.BLUE}{text}{Colors.END}")
    print(f"{Colors.BLUE}{'='*80}{Colors.END}\n")

def print_success(text: str):
    print(f"{Colors.GREEN}âœ“ {text}{Colors.END}")

def print_error(text: str):
    print(f"{Colors.RED}âœ— {text}{Colors.END}")

def print_info(text: str):
    print(f"{Colors.YELLOW}â„¹ {text}{Colors.END}")

def test_llm_health():
    """Test LLM health endpoint"""
    print_info("Testing LLM health endpoint...")
    try:
        response = requests.get(f"{LLM_URL}/health", timeout=5)
        response.raise_for_status()
        print_success(f"LLM health check passed")
        return True
    except Exception as e:
        print_error(f"LLM health check failed: {e}")
        return False

def test_llm_inference():
    """Test LLM code generation"""
    print_info("Testing LLM inference (code generation)...")

    payload = {
        "model": "qwen-coder",
        "messages": [
            {
                "role": "system",
                "content": "You are a helpful coding assistant."
            },
            {
                "role": "user",
                "content": "Write a Python function to calculate the factorial of a number using recursion."
            }
        ],
        "temperature": 0.7,
        "max_tokens": 512
    }

    try:
        response = requests.post(f"{LLM_URL}/v1/chat/completions", json=payload, timeout=60)
        response.raise_for_status()
        result = response.json()

        print_success("LLM inference successful!")
        print("\n" + "-"*80)
        print("Generated Code:")
        print("-"*80)
        print(result['choices'][0]['message']['content'])
        print("-"*80)
        print(f"Tokens used: {result['usage']['total_tokens']}")
        print(f"Model: {result['model']}")
        return True
    except Exception as e:
        print_error(f"LLM inference failed: {e}")
        return False

def test_llm_long_context():
    """Test LLM with long context (128K capable)"""
    print_info("Testing LLM long context capability...")

    # Create a longer prompt to test context
    long_text = "def example_function():\n    pass\n\n" * 100

    payload = {
        "model": "qwen-coder",
        "messages": [
            {
                "role": "user",
                "content": f"Analyze this code and count how many functions are defined:\n\n{long_text}"
            }
        ],
        "max_tokens": 256
    }

    try:
        response = requests.post(f"{LLM_URL}/v1/chat/completions", json=payload, timeout=60)
        response.raise_for_status()
        result = response.json()
        print_success(f"Long context test passed! Context tokens: ~{len(long_text)//4}")
        return True
    except Exception as e:
        print_error(f"Long context test failed: {e}")
        return False

def test_embeddings_health():
    """Test embeddings service health"""
    print_info("Testing embeddings service health...")
    try:
        response = requests.get(f"{EMBEDDING_URL}/health", timeout=5)
        response.raise_for_status()
        print_success("Embeddings health check passed")
        return True
    except Exception as e:
        print_error(f"Embeddings health check failed: {e}")
        return False

def test_embeddings_generation():
    """Test embeddings generation"""
    print_info("Testing embeddings generation...")

    payload = {
        "inputs": "This is a test sentence for embedding generation."
    }

    try:
        response = requests.post(f"{EMBEDDING_URL}/embed", json=payload, timeout=30)
        response.raise_for_status()
        embeddings = response.json()

        if isinstance(embeddings, list) and len(embeddings) > 0:
            vector_dim = len(embeddings[0])
            print_success(f"Embeddings generated successfully! Dimension: {vector_dim}")
            return True, vector_dim
        else:
            print_error("Invalid embeddings format")
            return False, 0
    except Exception as e:
        print_error(f"Embeddings generation failed: {e}")
        return False, 0

def test_qdrant_health():
    """Test Qdrant health"""
    print_info("Testing Qdrant health...")
    try:
        response = requests.get(f"{QDRANT_URL}/health", timeout=5)
        response.raise_for_status()
        print_success("Qdrant health check passed")
        return True
    except Exception as e:
        print_error(f"Qdrant health check failed: {e}")
        return False

def test_qdrant_operations(vector_dim: int):
    """Test Qdrant CRUD operations"""
    print_info("Testing Qdrant operations...")

    collection_name = "test_collection"

    try:
        # Create collection
        print_info(f"Creating collection '{collection_name}'...")
        response = requests.put(
            f"{QDRANT_URL}/collections/{collection_name}",
            json={
                "vectors": {
                    "size": vector_dim,
                    "distance": "Cosine"
                }
            },
            timeout=10
        )
        response.raise_for_status()
        print_success("Collection created")

        # Insert a test point
        print_info("Inserting test vector...")
        test_vector = [0.1] * vector_dim
        response = requests.put(
            f"{QDRANT_URL}/collections/{collection_name}/points",
            json={
                "points": [
                    {
                        "id": 1,
                        "vector": test_vector,
                        "payload": {"text": "test document"}
                    }
                ]
            },
            timeout=10
        )
        response.raise_for_status()
        print_success("Vector inserted")

        # Search
        print_info("Performing vector search...")
        response = requests.post(
            f"{QDRANT_URL}/collections/{collection_name}/points/search",
            json={
                "vector": test_vector,
                "limit": 5
            },
            timeout=10
        )
        response.raise_for_status()
        results = response.json()
        print_success(f"Search completed! Found {len(results.get('result', []))} results")

        # Clean up
        print_info("Cleaning up test collection...")
        requests.delete(f"{QDRANT_URL}/collections/{collection_name}", timeout=10)
        print_success("Test collection deleted")

        return True
    except Exception as e:
        print_error(f"Qdrant operations failed: {e}")
        # Try to clean up
        try:
            requests.delete(f"{QDRANT_URL}/collections/{collection_name}", timeout=5)
        except:
            pass
        return False

def test_integration():
    """Test integration: LLM + Embeddings + Qdrant"""
    print_info("Testing full integration pipeline...")

    try:
        # 1. Generate code with LLM
        print_info("Step 1: Generating code with LLM...")
        llm_response = requests.post(
            f"{LLM_URL}/v1/chat/completions",
            json={
                "model": "qwen-coder",
                "messages": [{"role": "user", "content": "Write a hello world function in Python"}],
                "max_tokens": 256
            },
            timeout=30
        )
        llm_response.raise_for_status()
        code = llm_response.json()['choices'][0]['message']['content']
        print_success(f"Code generated: {len(code)} characters")

        # 2. Create embedding
        print_info("Step 2: Creating embedding of generated code...")
        emb_response = requests.post(
            f"{EMBEDDING_URL}/embed",
            json={"inputs": code},
            timeout=30
        )
        emb_response.raise_for_status()
        embedding = emb_response.json()[0]
        print_success(f"Embedding created: dimension {len(embedding)}")

        # 3. Store in Qdrant
        print_info("Step 3: Storing in Qdrant...")
        collection_name = "integration_test"

        # Create collection
        requests.put(
            f"{QDRANT_URL}/collections/{collection_name}",
            json={"vectors": {"size": len(embedding), "distance": "Cosine"}},
            timeout=10
        )

        # Insert
        requests.put(
            f"{QDRANT_URL}/collections/{collection_name}/points",
            json={
                "points": [{
                    "id": 1,
                    "vector": embedding,
                    "payload": {"code": code}
                }]
            },
            timeout=10
        )

        # Search
        search_response = requests.post(
            f"{QDRANT_URL}/collections/{collection_name}/points/search",
            json={"vector": embedding, "limit": 1},
            timeout=10
        )
        search_response.raise_for_status()

        print_success("Full pipeline successful: LLM â†’ Embeddings â†’ Qdrant")

        # Clean up
        requests.delete(f"{QDRANT_URL}/collections/{collection_name}", timeout=5)

        return True
    except Exception as e:
        print_error(f"Integration test failed: {e}")
        return False

def main():
    print_header("AI Stack Comprehensive Test Suite")
    print_info("Testing: Qwen2.5-Coder-32B (128K) + Embeddings + Qdrant")

    results = {}

    # LLM Tests
    print_header("Testing LLM Service")
    results['llm_health'] = test_llm_health()
    time.sleep(1)
    results['llm_inference'] = test_llm_inference()
    time.sleep(1)
    results['llm_long_context'] = test_llm_long_context()

    # Embeddings Tests
    print_header("Testing Embeddings Service")
    results['embeddings_health'] = test_embeddings_health()
    time.sleep(1)
    success, vector_dim = test_embeddings_generation()
    results['embeddings_generation'] = success

    # Qdrant Tests
    print_header("Testing Qdrant Vector Database")
    results['qdrant_health'] = test_qdrant_health()
    time.sleep(1)
    if vector_dim > 0:
        results['qdrant_operations'] = test_qdrant_operations(vector_dim)
    else:
        results['qdrant_operations'] = False
        print_error("Skipping Qdrant operations (no vector dimension)")

    # Integration Test
    print_header("Testing Full Integration")
    results['integration'] = test_integration()

    # Summary
    print_header("Test Results Summary")
    for test_name, passed in results.items():
        status = f"{Colors.GREEN}âœ“ PASSED{Colors.END}" if passed else f"{Colors.RED}âœ— FAILED{Colors.END}"
        print(f"{test_name.replace('_', ' ').title()}: {status}")

    total = len(results)
    passed = sum(results.values())

    print(f"\n{Colors.BLUE}{'='*80}{Colors.END}")
    print(f"Total: {passed}/{total} tests passed")

    if passed == total:
        print_success("All tests passed! ðŸŽ‰")
        return 0
    else:
        print_error(f"{total - passed} test(s) failed")
        return 1

if __name__ == "__main__":
    sys.exit(main())
TEST_EOF

chmod +x test_services.py

# Create management script
print_status "Creating management script..."
cat > manage.sh <<'MANAGE_EOF'
#!/bin/bash

PROJECT_DIR="/opt/ai-stack"
cd $PROJECT_DIR

GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m'

case "$1" in
    start)
        echo -e "${GREEN}Starting AI Stack (LLM + Embeddings + Qdrant)...${NC}"
        docker compose up -d
        echo ""
        echo "Services starting..."
        echo "  - LLM (Qwen2.5-Coder-32B): http://localhost:8000"
        echo "  - Embeddings: http://localhost:8001"
        echo "  - Qdrant: http://localhost:6333"
        echo ""
        echo "View logs: ./manage.sh logs"
        echo "Check status: ./manage.sh status"
        ;;
    stop)
        echo -e "${YELLOW}Stopping AI Stack...${NC}"
        docker compose down
        ;;
    restart)
        echo -e "${YELLOW}Restarting AI Stack...${NC}"
        docker compose restart
        ;;
    logs)
        if [ -z "$2" ]; then
            docker compose logs -f
        else
            docker compose logs -f "$2"
        fi
        ;;
    status)
        docker compose ps
        echo ""
        echo "Health checks:"
        echo -n "  LLM: "
        curl -s http://localhost:8000/health >/dev/null 2>&1 && echo -e "${GREEN}âœ“ Healthy${NC}" || echo -e "${YELLOW}âœ— Not ready${NC}"
        echo -n "  Embeddings: "
        curl -s http://localhost:8001/health >/dev/null 2>&1 && echo -e "${GREEN}âœ“ Healthy${NC}" || echo -e "${YELLOW}âœ— Not ready${NC}"
        echo -n "  Qdrant: "
        curl -s http://localhost:6333/health >/dev/null 2>&1 && echo -e "${GREEN}âœ“ Healthy${NC}" || echo -e "${YELLOW}âœ— Not ready${NC}"
        ;;
    test)
        python3 test_services.py
        ;;
    pull)
        echo "Pulling latest images..."
        docker compose pull
        ;;
    clean)
        echo -e "${YELLOW}WARNING: This will remove all data and cached models!${NC}"
        read -p "Are you sure? (y/N) " -n 1 -r
        echo
        if [[ $REPLY =~ ^[Yy]$ ]]; then
            docker compose down -v
            echo "Cleaned up volumes and containers"
        fi
        ;;
    *)
        echo "AI Stack Management"
        echo ""
        echo "Usage: $0 {command} [options]"
        echo ""
        echo "Commands:"
        echo "  start          - Start all services"
        echo "  stop           - Stop all services"
        echo "  restart        - Restart all services"
        echo "  logs [service] - View logs (optional: qwen-llm, embeddings, qdrant)"
        echo "  status         - Check service status and health"
        echo "  test           - Run comprehensive test suite"
        echo "  pull           - Pull latest Docker images"
        echo "  clean          - Remove all data and cached models (WARNING: destructive)"
        echo ""
        echo "Examples:"
        echo "  $0 start"
        echo "  $0 logs qwen-llm"
        echo "  $0 test"
        exit 1
        ;;
esac
MANAGE_EOF

chmod +x manage.sh

# Create example usage script
print_status "Creating example usage scripts..."
cat > examples.py <<'EXAMPLES_EOF'
#!/usr/bin/env python3
"""
Example usage scripts for the AI Stack
"""
import requests
from openai import OpenAI

# ==========================================
# Example 1: Using LLM for code generation
# ==========================================
def example_llm():
    print("Example 1: Code Generation with 128K context LLM")
    print("="*80)

    client = OpenAI(
        base_url="http://localhost:8000/v1",
        api_key="not-needed"
    )

    response = client.chat.completions.create(
        model="qwen-coder",
        messages=[
            {"role": "system", "content": "You are a helpful coding assistant."},
            {"role": "user", "content": "Write a Python class for a binary search tree with insert and search methods."}
        ],
        temperature=0.7,
        max_tokens=1024
    )

    print(response.choices[0].message.content)
    print("="*80)

# ==========================================
# Example 2: Generate embeddings
# ==========================================
def example_embeddings():
    print("\nExample 2: Generate Embeddings")
    print("="*80)

    texts = [
        "Python is a programming language",
        "JavaScript is used for web development",
        "Machine learning uses neural networks"
    ]

    for text in texts:
        response = requests.post(
            "http://localhost:8001/embed",
            json={"inputs": text}
        )
        embedding = response.json()[0]
        print(f"Text: {text}")
        print(f"Embedding dimension: {len(embedding)}")
        print(f"First 5 values: {embedding[:5]}")
        print()

    print("="*80)

# ==========================================
# Example 3: Store and search in Qdrant
# ==========================================
def example_qdrant():
    print("\nExample 3: Vector Search with Qdrant")
    print("="*80)

    collection_name = "code_snippets"

    # Create collection
    requests.put(
        f"http://localhost:6333/collections/{collection_name}",
        json={
            "vectors": {
                "size": 1024,  # BGE-large dimension
                "distance": "Cosine"
            }
        }
    )

    # Add some code snippets
    snippets = [
        "def hello(): print('Hello World')",
        "function greet() { console.log('Hi'); }",
        "public void sayHello() { System.out.println('Hello'); }"
    ]

    points = []
    for i, snippet in enumerate(snippets):
        # Get embedding
        emb_response = requests.post(
            "http://localhost:8001/embed",
            json={"inputs": snippet}
        )
        embedding = emb_response.json()[0]

        points.append({
            "id": i + 1,
            "vector": embedding,
            "payload": {"code": snippet, "language": ["Python", "JavaScript", "Java"][i]}
        })

    # Insert points
    requests.put(
        f"http://localhost:6333/collections/{collection_name}/points",
        json={"points": points}
    )

    # Search for similar code
    query = "print hello message"
    query_emb_response = requests.post(
        "http://localhost:8001/embed",
        json={"inputs": query}
    )
    query_embedding = query_emb_response.json()[0]

    search_response = requests.post(
        f"http://localhost:6333/collections/{collection_name}/points/search",
        json={
            "vector": query_embedding,
            "limit": 3,
            "with_payload": True
        }
    )

    results = search_response.json()['result']
    print(f"Query: '{query}'")
    print(f"\nTop {len(results)} similar code snippets:")
    for i, result in enumerate(results, 1):
        print(f"\n{i}. Score: {result['score']:.4f}")
        print(f"   Language: {result['payload']['language']}")
        print(f"   Code: {result['payload']['code']}")

    print("\n" + "="*80)

# ==========================================
# Example 4: Full RAG pipeline
# ==========================================
def example_rag():
    print("\nExample 4: RAG (Retrieval-Augmented Generation)")
    print("="*80)

    # This combines all three services for a RAG workflow

    # 1. User asks a question
    question = "How do I reverse a string in Python?"

    # 2. Get embedding of question
    q_emb = requests.post(
        "http://localhost:8001/embed",
        json={"inputs": question}
    ).json()[0]

    # 3. Search Qdrant for relevant code (assuming collection exists)
    # ... search logic ...

    # 4. Use LLM to generate answer with context
    client = OpenAI(base_url="http://localhost:8000/v1", api_key="not-needed")

    response = client.chat.completions.create(
        model="qwen-coder",
        messages=[
            {"role": "user", "content": question}
        ]
    )

    print(f"Question: {question}")
    print(f"\nAnswer:\n{response.choices[0].message.content}")
    print("="*80)

if __name__ == "__main__":
    print("AI Stack Usage Examples")
    print("Make sure all services are running: ./manage.sh status\n")

    try:
        example_llm()
        example_embeddings()
        example_qdrant()
        example_rag()

        print("\nâœ“ All examples completed successfully!")
    except Exception as e:
        print(f"\nâœ— Error: {e}")
        print("Make sure all services are running: ./manage.sh start")
EXAMPLES_EOF

chmod +x examples.py

print_section "Installation Complete!"

print_status ""
print_status "========================================="
print_status "ðŸŽ‰ AI Stack Setup Complete!"
print_status "========================================="
print_status ""
print_status "Installed components:"
print_status "  âœ“ Docker + NVIDIA Container Toolkit"
print_status "  âœ“ vLLM (for LLM inference)"
print_status "  âœ“ HuggingFace Text Embeddings Inference"
print_status "  âœ“ Qdrant Vector Database"
print_status ""
print_status "Installation directory: $PROJECT_DIR"
print_status ""
print_status "Configuration:"
print_status "  - LLM: Qwen2.5-Coder-32B-Instruct (128K context window!)"
print_status "  - Embeddings: BAAI/bge-large-en-v1.5 (1024 dimensions)"
print_status "  - Vector DB: Qdrant (latest)"
print_status ""
print_status "Next steps:"
print_status ""
print_status "1. (Optional) Add HuggingFace token for faster downloads:"
print_status "   nano $PROJECT_DIR/.env"
print_status "   # Add: HF_TOKEN=hf_your_token_here"
print_status "   # Get token: https://huggingface.co/settings/tokens"
print_status ""
print_status "2. Start all services:"
print_status "   cd $PROJECT_DIR"
print_status "   ./manage.sh start"
print_status ""
print_status "3. Watch the logs (first run downloads ~70GB of models):"
print_status "   ./manage.sh logs"
print_status ""
print_status "4. Check service status:"
print_status "   ./manage.sh status"
print_status ""
print_status "5. Run comprehensive tests:"
print_status "   ./manage.sh test"
print_status ""
print_status "6. Try example usage:"
print_status "   python3 examples.py"
print_status ""
print_status "Service URLs:"
print_status "  - LLM API: http://localhost:8000"
print_status "  - Embeddings API: http://localhost:8001"
print_status "  - Qdrant UI: http://localhost:6333/dashboard"
print_status ""
print_status "Management commands:"
print_status "  ./manage.sh start       - Start all services"
print_status "  ./manage.sh stop        - Stop all services"
print_status "  ./manage.sh restart     - Restart all services"
print_status "  ./manage.sh logs        - View all logs"
print_status "  ./manage.sh logs qwen-llm   - View LLM logs only"
print_status "  ./manage.sh status      - Check health of all services"
print_status "  ./manage.sh test        - Run comprehensive tests"
print_status ""
print_status "Key Features:"
print_status "  âœ“ 128K context window (one of the largest available!)"
print_status "  âœ“ Optimized for your 80GB GPU"
print_status "  âœ“ OpenAI-compatible API"
print_status "  âœ“ Full RAG capability (LLM + Embeddings + Vector DB)"
print_status "  âœ“ Production-ready with health checks"
print_status ""
print_status "========================================="
